{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXPLICACION DEL TRABAJO DE PROCESADO DEL DATA SET DE PELICULAS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CONSIGNA:\n",
    "\n",
    " Deberá adaptar los datos de la siguiente forma:\n",
    " - Descartar las películas que no tienen “overview”.\n",
    " - Descartar las películas cuyo idioma original tenga más de 2 caracteres.\n",
    " - Tomar las 100 palabras más comunes de todos los overviews combinados.\n",
    " - Para generar el ”Overview” de cada película para el archivo de salida se deberán\n",
    " descartar las palabras que se encuentren dentro de las 100 más comunes de todos los\n",
    " overviews y se deberá generar un string con 3 palabras tomadas al azar y ordenadas al\n",
    " azar (ver random.sample()) de ese resultado. Nota: luego de filtrar las palabras\n",
    " comunes algunos overviews pueden tener menos de 3 palabras, en esos casos se\n",
    " tomarán las palabras que se pueda.\n",
    " - Se utilizarán como datos de las tarjetas: “Genre”, “Original_Language”, “Vote_Average”,\n",
    " “Release_Date” y 3 palabras aleatorias del overview separadas por “;”. Como dato a\n",
    " adivinar se utilizará “Title”. Descartar el resto de las columnas.\n",
    " - El archivo resultante deberá tener las siguientes columnas (en este orden específico):\n",
    " “Genre”, “Original_Language”, “Release_Date”, “Vote_Average”, “Overview” y “Title”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### URL: https://www.kaggle.com/datasets/disham993/9000-movies-dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMPLEMENTACION DE LA RESOLUCION:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero vamos a importar las librerias a utilizar que van a ser necesarias para el procesamiento de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "from collections import Counter\n",
    "import random\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora vamos a empezar la lectura del csv buscando la ruta del directorio actual donde se encuentra el archivo donde contiene la lista de peliculas necesarias para lo primero que pide la consigna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ubicacion = os.path.join(\"folder_csv\")\n",
    "carpeta_actual = os.path.join(\"src\")\n",
    "carpeta_sig = os.path.join(\"core\")\n",
    "ruta_act = os.path.join(os.getcwd(),carpeta_actual,carpeta_sig) \n",
    "ruta_final = os.path.join(ruta_act,ubicacion)\n",
    "\n",
    "with open(os.path.join(ruta_final,'mymoviedb.csv'), \"r\",encoding=\"utf8\") as logs_peliculas:\n",
    "     lector = csv.reader(logs_peliculas, delimiter=\",\")\n",
    "     header, datos = next(lector), list(lector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizamos el with ya que permite cerrar el archivo sin poner el open y utilizamos el path.join para unir la direccion del directorio actual donde se encuentra el archivo en donde se ubica. Tambien vamos a quitar del encabezado por asi decirlo para poder manejar los datos en bruto mas facil el manejo de los datos. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora vamos a juntar todas las overviews de las peliculas en una sola lista para poder generar la lista de las 100 palabras mas comunes en todas las overwievs del csv:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "     word_list = []\n",
    "     for linea in datos:\n",
    "         cadena = linea[2].split()\n",
    "         for palabra in cadena: #Recorro la cadena actual de palabras\n",
    "             word_list.append(palabra)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y ahora vamos a buscar las palabras mas comunes que se encuentran en todos los overviews utilizando el iterador Counter que contiene el most_commons() que busca coincidencias "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "     word_list = list(dict(Counter(word_list).most_common(100)).keys()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como ya tenemos la lista de las 100 palabras que mas aparecen en los overviews,ahora vamos a realizar un proceso para generar 3 palabras al azar unidas por ; que van a ser provistas al nuevos csv cuando se hayan creado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "     def generar_cadena(cadena):\n",
    "         \"\"\"Busca palabra por palabra en la cadena de la linea de overviews y va descartando del la \n",
    "         linea si la palabra actual esta en la word list que contiene las 100 palabras que aparecen \n",
    "         en los overviews combinados\"\"\"\n",
    "         data = []\n",
    "         for palabra,word in zip(cadena,word_list):\n",
    "             if palabra != word:\n",
    "                 data.append(palabra)\n",
    "         data = \";\".join(random.sample(data,min(len(data),3)))\n",
    "         return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data es la lista que son generadas a partir del descarte de palabras que se encuentran en la lista combinada y que tambien estan en la overview actual. A lo ultimo utilizamos el sample por que devuelve una muestra de 3 palabras. Se utilizo el min(tamaño de la lista actual,3) sobre sample por que genera excepciones ValueError y esta solucion evita que suceda esos errores ya que podia haber muchas muestras negativas. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora vamos a realizar la creacion del csv utilizando un diccionario y DictWriter para escribir sobre el. Primero realizamos la obtencion de la lista completa con los paises cuyo lenguaje consiste en 2 caracteres junto con descartar la cantidad que haya de peliculas sin overviews y luego vamos a usar la cadena actual para generar la lista de 3 palabras al azar con generar_cadena()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "     refined_data_list = []\n",
    "     for linea in datos:\n",
    "         idioma = len(linea[6])\n",
    "         cadena = linea[2].strip(\",\").split() #Lista de palabras de la cadena actual del overview\n",
    "         if idioma == 2 and len(linea[2]) > 0:\n",
    "             refined_data_list.append({'Genre': linea[7], 'Original_Language': linea[6],'Release_Date': linea[0],'Vote_Average': linea[5],'Overview': generar_cadena(cadena),'Title': linea[1]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "refined_data_list es donde se van a almacenar todos los datos resultantes que van a ir al csv y como es una lista de diccionarios va a simplificar la escritura del csv ya que recorro menos el archivo. Ahora vamos a generar el csv donde estan los resultados llamado  **peliculas_figurace** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a utilizar pandas para generar los datos.\n",
    "Ahora establecemos el nombre de los campos con la lista llamada field_names y escribimos su encabezado y para finalizar escribirmos el encabezado que seria las palabras en field_names y la lista que se genero anteriormente conocida como refined_data_list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "field_names =['Genre','Original_Language','Release_Date','Vote_Average','Overview','Title']\n",
    "df = pd.DataFrame(refined_data_list,columns=field_names)\n",
    "df.to_csv(os.path.join(os.getcwd(),ruta_final,'peliculas_figurace.csv'), index=False) "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
